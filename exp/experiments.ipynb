{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Llama 3.1 8B Instruct Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to start by importing all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlieclark/miniconda3/envs/medical-chatbot/lib/python3.9/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_ollama import OllamaLLM\n",
    "import pinecone\n",
    "\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Langchain Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize a simple test prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Please give me a trivia fact about the {model_name} deep learning model.'''\n",
    "prompt = PromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should initialize an instance of Meta's Llama 3.1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model='llama3.1:8b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should create a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we should invoke the chain using some input argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a trivia fact:\n",
      "\n",
      "The Multilayer Perceptron (MLP), also known as a feedforward neural network, was first introduced in a 1943 paper by Warren McCulloch and Walter Pitts, two neuroscientists who proposed the basic architecture of artificial neurons and their connections. However, the modern MLP model that we know today, with its backpropagation training algorithm, was popularized by David Rumelhart, Geoffrey Hinton, and Yann LeCun in their 1986 paper \"Backpropagation: Theory, Architectures, and Applications\".\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Multilayer Perceptron (MLP)' # change to whatever your preferred DL model is\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "answer = chain.invoke({'model_name': model_name})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function that extracts data from a PDF file and then use it on the Medical Encyclopedia to be used as the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path: str) -> List[Document]:\n",
    "    '''\n",
    "    Extracts data from the PDF at the passed path.\n",
    "\n",
    "    Inputs:\n",
    "        path: a filepath to the PDF to be extracted from.\n",
    "\n",
    "    Returns:\n",
    "        docs: a list of Document objects containing the extracted data.\n",
    "    '''\n",
    "    \n",
    "    loader = PyPDFDirectoryLoader(path=path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_data('../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a function to split the extracted data into text chunks and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(data: List[Document]) -> List[Document]:\n",
    "    '''\n",
    "    Splits the input data into text chunks.\n",
    "\n",
    "    Inputs:\n",
    "        data: a list of Document objects containing data extracted from a PDF.\n",
    "\n",
    "    Returns:\n",
    "        chunks: a list of Document objects containing text chunks.\n",
    "    '''\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    chunks = splitter.split_documents(data)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we need to download an embedding model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlieclark/miniconda3/envs/medical-chatbot/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to populate our Pinecone Index with embedded text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'medical-chatbot'\n",
    "pc_vector_store = PineconeVectorStore.from_documents(chunks, index_name=index_name, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should create our prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "Please use the following information to answer the user's question.\n",
    "If you don't know the answer, do NOT try to make one up; just say you don't know.\n",
    "\n",
    "If the user thanks you, give a typical response.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return a helpful answer and nothing else below:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=['context', 'question'])\n",
    "chain_kwargs = {'prompt': prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we will set up our LLM and our chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model='llama3.1:8b', temperature=0.8, num_gpu=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=pc_vector_store.as_retriever(), return_source_documents=True, chain_type_kwargs=chain_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: What is the cause of heart attacks?\n",
      "Response: Coronary artery disease, caused by an accumulation of fatty materials on the inner linings of arteries (atherosclerosis), leading to blocked or restricted blood flow to the heart, resulting in a heart attack.\n",
      "\n",
      "Input: What is a TIA?\n",
      "Response: A transient ischemic attack (TIA), also known as a mini-stroke, is a disruption in the blood supply to the brain caused by a blocked or burst blood vessel. This results in a temporary impairment of vision, speech, or movement that usually lasts for just a few moments. It may be a warning sign for a full-scale stroke.\n",
      "\n",
      "Input: Is there a cure for AIDS?\n",
      "Response: There is no cure for AIDS, but with antiretroviral therapy (ART) and other treatments, people living with HIV/AIDS can manage the virus, reduce their viral load to undetectable levels, and live long, healthy lives. However, if left untreated or without proper management, the disease will progress, leading to the symptoms and complications associated with AIDS.\n",
      "\n",
      "Input: What are the symptoms of bubonic plague?\n",
      "Response: Unfortunately, this information does not mention bubonic plague. The provided text discusses various parasitic infections (fascioliasis, opisthorchiasis, clonorchiasis, and filariasis) but does not cover bubonic plague or its symptoms.\n",
      "\n",
      "Input: I have the following symptoms: sore throat, pain when swallowing, fever, chills, and nausea. What are the top 3 most likely diagnoses?\n",
      "Response: Based on your symptoms, the top 3 most likely diagnoses are:\n",
      "\n",
      "1. Strep throat (sore throat caused by Streptococcus bacteria)\n",
      "2. Tonsillitis (inflammation of a tonsil)\n",
      "3. A secondary bacterial infection that needs to be treated with an antibiotic, possibly resulting from a viral illness such as COVID-19 or another virus\n",
      "\n",
      "Input: Thank you\n",
      "Response: You're welcome!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = ''\n",
    "while question.lower() != 'thank you':\n",
    "    question = input('Enter your medical question or \"Thank you\" to exit: ')\n",
    "    print(f'Input: {question}')\n",
    "\n",
    "    result = qa({'query': question})\n",
    "    print(f'Response: {result[\"result\"]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
