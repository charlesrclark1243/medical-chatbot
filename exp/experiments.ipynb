{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Llama 3.1 8B Instruct Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to start by importing all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.llms.ollama import Ollama\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Langchain Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize a simple test prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Please give me a trivia fact about the {model_name} deep learning model.'''\n",
    "prompt = PromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should initialize an instance of Meta's Llama 3.1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ollama(model='llama3.1:8b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should create a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we should invoke the chain using some input argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a trivia fact:\n",
      "\n",
      "The Vision Transformer (ViT) model, which was introduced in the paper \"An Image is Worth More than 16x16 Words: Tri-attention with Dual Scale\" (later revised to \"Transformers in Vision and Video Understanding\") by Alexey Dosovitskiy et al. in 2021, achieved state-of-the-art results on several image classification benchmarks at its introduction.\n",
      "\n",
      "In particular, it achieved a Top-1 accuracy of 83.2% on ImageNet validation set, surpassing the previous best result of 81.3% by Swin Transformer (a concurrent work). This achievement marked a significant milestone in the field of computer vision and demonstrated the potential of transformer models for visual tasks.\n",
      "\n",
      "Do you want to know more about ViT or is there something else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "answer = chain.invoke({'model_name': 'Vision Transformer'})\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
