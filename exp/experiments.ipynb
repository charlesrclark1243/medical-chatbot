{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Llama 3.1 8B Instruct Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to start by importing all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_ollama import OllamaLLM\n",
    "import pinecone\n",
    "\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Langchain Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize a simple test prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Please give me a trivia fact about the {model_name} deep learning model.'''\n",
    "prompt = PromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should initialize an instance of Meta's Llama 3.1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model='llama3.1:8b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should create a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we should invoke the chain using some input argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a trivia fact:\n",
      "\n",
      "**Did you know that the Multilayer Perceptron (MLP) was actually invented by Frank Rosenblatt in 1958, decades before the modern Deep Learning era?**\n",
      "\n",
      "Rosenblatt, an American computer scientist and engineer at Cornell Aeronautical Laboratory, published a paper titled \"The Perceptron: A Perceiving Machine\" in which he described the concept of a feedforward neural network with multiple layers (the MLP). This was a groundbreaking work that laid the foundation for many modern deep learning techniques.\n",
      "\n",
      "It's interesting to note that while Rosenblatt's work predated the resurgence of interest in neural networks, his own perceiver was not as effective or scalable as modern MLPs. Nevertheless, his contribution to the field is undeniable!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Multilayer Perceptron' # change to whatever your preferred DL model is\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "answer = chain.invoke({'model_name': model_name})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function that extracts data from a PDF file and then use it on the Medical Encyclopedia to be used as the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path: str) -> List[Document]:\n",
    "    '''\n",
    "    Extracts data from the PDF at the passed path.\n",
    "\n",
    "    Inputs:\n",
    "        path: a filepath to the PDF to be extracted from.\n",
    "\n",
    "    Returns:\n",
    "        docs: a list of Document objects containing the extracted data.\n",
    "    '''\n",
    "    \n",
    "    loader = PyPDFDirectoryLoader(path=path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_data('../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a function to split the extracted data into text chunks and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(data: List[Document]) -> List[Document]:\n",
    "    '''\n",
    "    Splits the input data into text chunks.\n",
    "\n",
    "    Inputs:\n",
    "        data: a list of Document objects containing data extracted from a PDF.\n",
    "\n",
    "    Returns:\n",
    "        chunks: a list of Document objects containing text chunks.\n",
    "    '''\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    chunks = splitter.split_documents(data)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we need to download an embedding model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlieclark/miniconda3/envs/medical-chatbot/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to populate our Pinecone Index with embedded text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'medical-chatbot'\n",
    "pc_vector_store = PineconeVectorStore.from_documents(chunks, index_name=index_name, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should create our prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "Please use the following information to answer the user's question.\n",
    "If you don't know the answer, do NOT try to make one up; just say you don't know.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return a helpful answer and nothing else below:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=['context', 'question'])\n",
    "chain_kwargs = {'prompt': prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we will set up our LLM and our chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model='llama3.1:8b', temperature=0.8, num_gpu=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=pc_vector_store.as_retriever(), return_source_documents=True, chain_type_kwargs=chain_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: What are the symptoms of the flu?\n",
      "Response: Fever, headache, body ache, sore throat, chills, weakness, and low-grade fever followed by watery diarrhea, nausea, loss of appetite, and muscle aches.\n",
      "\n",
      "Input: What are the symptoms of the bubonic plague?\n",
      "Response: I don't see any information on the bubonic plague in the provided text.\n",
      "\n",
      "Input: What about strep throat? What are the symptoms for that?\n",
      "Response: A sore throat, chills, fever, and swollen lymph nodes in the neck.\n",
      "\n",
      "Input: thank you\n",
      "Response: You're welcome.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = ''\n",
    "while question.lower() != 'thank you':\n",
    "    question = input('Enter your medical question or \"Thank you\" to exit: ')\n",
    "    print(f'Input: {question}')\n",
    "\n",
    "    result = qa({'query': question})\n",
    "    print(f'Response: {result[\"result\"]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
